---
published: true
title: "# NFS - Network File System \U0001F5A7"
tags: nfs network filesystem nas samba linux-system
toc: true
---
> In a closed network (where you know every device), NFS is a fine choice. With a good network, throughput it disgustingly fast and at the same time less CPU intensive on the server. It's very simple to set up and you can toggle readonly on shares you don't need to be writeable. - [NFS vs Samba](https://askubuntu.com/questions/7117/which-to-use-nfs-or-samba#7124)

<link rel="shortcut icon" href="https://cdn.iconscout.com/icon/premium/png-512-thumb/network-folder-10-794131.png?f=webp&w=512" type="image/x-icon" />

## [NFS & Symlink](https://unix.stackexchange.com/questions/135048/how-to-follow-symlinks-in-nfs/460929#460929)

Symbolic links only contain a path to another file or directory on the originating system where they're being shared from. Unless you take care to make the links relative or to duplicate the same directory structures on remote systems as the originator of the share, they simply will not work.

This can be easily fixed: see [symlink]({% post_url 2021-10-18-symlink %})


# Client Configuration ðŸ’»

## Basics

### List shared for a given server
{% highlight bash %}
$ showmount -e [server ip]
{% endhighlight %}
  
### Mount Shared Directory (Manual)
{% highlight bash %}
$ sudo mkdir -p /nfs/backup
$ sudo mount 192.168.0.125:/mnt/Backup /nfs/backup
{% endhighlight %}

or
- use [permanent mount](#permanent-mount)
- use autofs / automount (see below)

### Mount a SubTree
see [ChatGPT](https://chatgpt.com/share/681f1769-bbd4-800d-ab37-38e33c34ff2f)


## Auto-mounting an NFS share

**autofs**, with the right auto scripts, will dynamically list the available shares. So you don't need to pre-define and hard-code which machines/shares should be made available.

While with [**systemd's automount**](#systemd-mount-unit), only shares which you have pre-configured will be visible.


## Install

{% highlight bash %}
$ sudo apt install nfs-common
{% endhighlight %}

### Autofs Setup

- [ubuntu](https://help.ubuntu.com/community/Autofs)
- [How to use autofs](https://opensource.com/article/18/6/using-autofs-mount-nfs-shares)
- [stack](https://askubuntu.com/questions/884389/auto-mount-nfs-via-autonfs)

{% highlight bash %}
$ sudo apt-get install autofs
{% endhighlight %}

Add the following line at the end of [**/etc/auto.master**](https://doc.ubuntu-fr.org/autofs) 

{% highlight ini %}
/nfs   /etc/auto.nfs	--ghost,--timeout=30
{% endhighlight %}

Create **/etc/auto.nfs**, and specify [nfs mount options](http://manpages.ubuntu.com/manpages/bionic/man5/nfs.5.html):
{% highlight ini %}
<server-name>   -fstype=nfs4,soft   <server-ip/host>:/
  
tronaut   -fstype=nfs4,soft   tronaut:/
{% endhighlight %}

Reload /etc/init.d/autofs
{% highlight bash %}
$ systemctl start autofs

# now test that it works
$ ls /nfs/tronaut/  # pressing tab should show shared available
{% endhighlight %}
  
see also [FSCache](#use-fs-cache--cachefilesd-with-nfs) to minimize network traffic for users accessing data from a file system mounted over the network.

## [Find Usage](https://chatgpt.com/share/697e09f2-fa20-800d-af60-ab0dfb6ce807)

Lookup for usage of nfs mount on a given client, by looking through potential configuration.

{% highlight bash %}
# fstab
$ grep -R "server1:/exports/data" /etc/fstab
$ grep -R fstab /etc
  
# systemd mount units
$ systemctl list-unit-files --type=mount   # explicit mount
$ find /etc/systemd -type f | xargs grep -H "server1" # drop ins
  
# autofs
$ grep -R "server1" /etc/auto*
$ grep -R "exports/data" /etc/auto*
  
# systemd automount units
$ systemctl list-unit-files --type=automount
$ systemctl list-units --type=automount --all
$ systemctl cat <unit>.automount
{% endhighlight %}
  
  
# [Host Configuration](https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-18-04#step-3-%E2%80%94-configuring-the-nfs-exports-on-the-host-server) ðŸ–¥

## Install

{% highlight bash %}
$ sudo apt install nfs-kernel-server
{% endhighlight %}
  
`sudo micro` **/etc/exports**
{% highlight text %}
directory_to_share    client(share_option1,...,share_optionN)
{% endhighlight %}
  
Then restart service: `sudo systemctl restart nfs-kernel-server`
  
## Example

Setup user specific folder on NAS

**On Server** [create ZFS dataset]({% post_url 2019-03-31-zfs-ubuntu %}#2---create-zfs-filesystem-or-dataset) / [NFS Share]()
{% highlight bash %}
$ zfs create -o mountpoint=/mnt/tronaut-yves storage_pool/tronaut-yves

# add it to NFS exports (/etc/exports)
$ sudo micro /etc/exports
# /mnt/tronaut-yves *(rw,sync,no_subtree_check,no_root_squash)

$ sudo systemctl restart nfs-kernel-server 
  
# check export
$ sudo exportfs -v
{% endhighlight %}

### Options details
** * ** Allow all clients to connect, otherwise you can specify clients (by IP, hostname, subnet or domain).

**rw:** This option gives the client computer both read and write access to the volume.
  
**sync:** This option forces NFS to write changes to disk before replying. This results in a more stable and consistent environment since the reply reflects the actual state of the remote volume. However, it also reduces the speed of file operations.
  
**no_subtree_check:** This option prevents subtree checking, which is a process where the host must check whether the file is actually still available in the exported tree for every request. This can cause many problems when a file is renamed while the client has it opened. In almost all cases, it is better to disable subtree checking.
  
**no_root_squash:** By default, NFS translates requests from a root user remotely into a non-privileged user on the server. This was intended as security feature to prevent a root account on the client from using the file system of the host as root. no_root_squash disables this behavior for certain shares.

- [How to properly set permissions for NFS folder?](https://serverfault.com/questions/240897/how-to-properly-set-permissions-for-nfs-folder-permission-denied-on-mounting-en)

### see also
- [Export a partial subtree](https://chatgpt.com/share/681f1769-bbd4-800d-ab37-38e33c34ff2f) - use bind mount to only export a subtree through NFS.
  
## Refs
- [How To Set Up an NFS Mount](https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-18-04)
- [Ubuntu](https://help.ubuntu.com/community/SettingUpNFSHowTo#Shares)
- [NFS Client Configuration To Mount NFS Share](https://www.cyberciti.biz/tips/ubuntu-linux-nfs-client-configuration-to-mount-nfs-share.html)
  

# [Fs-cache](https://chatgpt.com/share/681c8f76-b64c-800d-95e0-9f5f8a2605c5)

<div style="
  border-left: 5px solid #fb8c00; /* orange */
  background: #fff3e0;
            
  padding: 1rem;
  margin: 1rem 0;
  border-radius: 6px;
"  markdown="1" >

This can improve speed, principal goal is to decrease network traffic.

**It does not give offline access to data** - no onedrive.

</div>

Needs [FS-Cache](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-fscache#ch-fscache) needs to be installed locally

{% highlight bash %}
$ sudo apt install cachefilesd
{% endhighlight %}

And it must be enabled `/etc/default/cachefilesd` -> `RUN=yes`
and then `sudo systemctl enable --now cachefilesd`

Now when mounting nfs, use **`-o fsc`** to enables FS-Cache support, eg:  
`sudo mount -o fsc yourserver:/yourpath /mnt/nfs`

## Permanent Mount

**Avoid? Autofs mount for permanent mount**

- because if you do the content is unreliable, the mount point having been removed.
- **do not share** mount point between autofs and cachedfs (do not use `/nfs` if this is the autofs mount point)

**On Client**
{% highlight bash %}
$ mkdir yves ~/Tronaut

# manual mount
$ sudo mount -t nfs4 -o fsc tronaut:/mnt/tronaut-yves /home/yves/Tronaut
{% endhighlight %}

or create a systemd mount
  
### [systemd mount unit](https://chatgpt.com/share/681ce740-8f10-800d-b2e3-ab9d8627159e)

**systemd .mount unit filenames must be the escaped version of the mount path.**

{% highlight bash %}
$ systemd-escape -p --suffix=mount /home/yves/Tronaut
=> home-yves-Tronaut.mount
{% endhighlight %}

in `/etc/systemd/system/home-yves-Tronaut.mount`
{% highlight init %}
# /etc/systemd/system/home-yves-Tronaut.mount
[Unit]
Description=Mount Tronaut NFS Share for yves
After=network-online.target
Wants=network-online.target

[Mount]
What=tronaut:/mnt/tronaut-yves
Where=/home/yves/Tronaut
Type=nfs4
Options=_netdev,fsc,auto,nofail,x-systemd.automount,x-systemd.device-timeout=10,timeo=14

[Install]
WantedBy=multi-user.target
{% endhighlight %}

{% highlight bash %}
$ sudo systemctl daemon-reexec
$ sudo systemctl enable home-tronaut-yves-mnt-share.mount
$ sudo systemctl start home-yves-Tronaut.mount
{% endhighlight %}

### Disabling mount.unit

To remove the configuration above.

{% highlight bash %}
$ systemctl list-units --type=mount
or
$ systemctl list-unit-files --type=mount

$ sudo systemctl stop <mnt-data>.mount
$ sudo systemctl disable <mnt-data>.mount
$ sudo rm /etc/systemd/system/<mnt-data>.mount
  
$ sudo systemctl daemon-reload
{% endhighlight %}
  
**see also**  
- [Automatic mounts with systemd](https://blog.tomecek.net/post/automount-with-systemd/)
	- [Systemd automount vs autofs](https://unix.stackexchange.com/questions/374103/systemd-automount-vs-autofs#375602)

## [Config](https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/)

`sudo xed /etc/cachefilesd.conf`

Default backend is in `/var/cache/fscache`

There are 6 control in `/etc/cachefilesd.conf` Configure Cache culling:
**brun N%** (percentage of blocks) & **frun N%** (percentage of files): this describes the amount of free space and the number of available files. If these values cache rise above the set percentage, then culling is turned off  
**bcull N%** (percentage of blocks) & **fcull N%** (percentage of files): describes the amount of available space or the number of files and if these values fall below the set limit, then cull is started.
**bstop N%** (percentage of blocks) & **fstop N%** (percentage of files): here if the amount of available space or the number of available files in the cache falls below either of these limits then the allocation of disk space stops until the limits are raised above the set percentage.

[![fscace design](https://computingforgeeks.com/wp-content/uploads/2022/07/Cache-NFS-Share-Data-with-FS-Cache-1.png?ezimgfmt=ng:webp/ngcb23)](https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/)
  
# [Handling host unavailability](https://chatgpt.com/share/68206c39-8b40-800d-8dfd-3a05e6cfac02)

When an NFS share becomes unavailable, it will cause any application trying to access the files to hang (including ls).
  
## systemd automount

**hard** retry failed requests indefinitely until the server responds â€” effectively blocking until success.
**timeo=5** Timeout in tenths of a second (i.e., 0.5 seconds).  
**retrans=2** Retry a request this many times before giving up.  
**intr**  The intr option in NFS mount options allows NFS requests to be interrupted if the server becomes unresponsive. This means that system calls like ls, cat, or even umount on the NFS mount can be interrupted with signals like Ctrl+C or SIGTERM.  
**x-systemd.device-timeout**  Configure how long systemd should wait for the mount command to finish before giving up on an entry from /etc/fstab. Specify a time in seconds or explicitly append a unit such as "s", "min", "h", "ms".

Use systemd's automounting to mount only when accessed.
  
Example systemd mount unit
{% highlight ini %}
[Mount]
What=nfsserver:/export/path
Where=/mnt/nfs
Type=nfs
Options=_netdev,fsc,auto,nofail,x-systemd.device-timeout=10,soft,timeo=5,retrans=3,intr
TimeoutSec=30
{% endhighlight %}
  

### see also
- [How can I cache NFS shares on a local disk?](https://askubuntu.com/questions/4572/how-can-i-cache-nfs-shares-on-a-local-disk)
- [Can FS-Cache be configured to always cache the full file on read?](https://unix.stackexchange.com/questions/389349/can-fs-cache-be-configured-to-always-cache-the-full-file-on-read)
- ["sync" command over NFS](https://stackoverflow.com/questions/21413048/sync-command-over-nfs) - Consider just `sudo mount /nfs-mount -o remount
- [How to unmount NFS when server is gone?](https://askubuntu.com/questions/292043/how-to-unmount-nfs-when-server-is-gone) - `umount -f -l /mnt/myfolder`, and that will fix the problem.
- [Why is Linux NFS server implemented in the kernel as opposed to userspace?](https://unix.stackexchange.com/questions/45899/why-is-linux-nfs-server-implemented-in-the-kernel-as-opposed-to-userspace)

### [Mapping user between system](https://chatgpt.com/share/68604ac1-3c40-800d-adb0-f9e6de254d44)

NFS relies on UID and GID matching between the client and server to manage file ownership. If they don't match, you'll see incorrect ownership (e.g., nobody or wrong users).

If this is not possible, `idmapd` can be used in **NFSv4**, it allows map usernames instead of UIDs within a NFS defined _domain_. The _domain_ value must be identical on both client and server. **It does not have to match DNS** â€” it's just a logical namespace for NFS mapping.

# See also
- [	NFS at 40 â€“ Remembering the Sun Microsystems Network File System](https://news.ycombinator.com/item?id=45482467) - What I learned though was that NFS was great until it wasn't. If the server hung, all work stopped.
	- [Sheds a tear for AFS](https://news.ycombinator.com/item?id=45486081) - [Andrew File System](https://chatgpt.com/share/6923617a-f2a0-800d-ac77-7df663ca5276) -  We had a nice, distributed file system that even had solid security and didn't fail in these silly ways--everybody ignored it.
		- [AFS implements weak consistency](https://news.ycombinator.com/item?id=45486221) - which may be a bit surprising. It also seems to share objects, not block devices. Judging by its features, it seems to make most sense when there is a cluster of servers. It looks cool though, a bit more like S3 than like NFS.
	- [openafs.org](https://www.openafs.org/main.html)
		- [OpenAFS and the Dawn of a New Era (Morgan Stanley)](https://workshop.openafs.org/afsbpw08/talks/wed_1/OpenAFS_and_the_Dawn_of_a_New_Era.pdf)
